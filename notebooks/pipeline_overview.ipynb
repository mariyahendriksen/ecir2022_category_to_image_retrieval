{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP data processing pipeline:\n",
    "- [ ] Convert textual and visual data into a dict\n",
    "- [ ] Create train-test-dev splits\n",
    "- [ ] Train model on train data\n",
    "- [ ] Test the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('/Users/mhendriksen/Desktop/repositories/CLIP/src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CUB_csv_prep.ipynb',\n",
       " 'best_model_CUB_30E_128BS.pt',\n",
       " 'best_model_CUB_30E_128BS_image_to_text.pkl',\n",
       " 'CUB_200_2011']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "root = '/Users/mhendriksen/Desktop/repositories/datasets/CUB_200_2011/'\n",
    "\n",
    "DATASET = 'CUB'\n",
    "\n",
    "files = os.listdir(root)\n",
    "\n",
    "files_filtered = [file for file in files if DATASET in file]\n",
    "\n",
    "files_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from  /Users/mhendriksen/Desktop/repositories/datasets/CUB_200_2011/best_model_CUB_30E_128BS_image_to_text.pkl\n"
     ]
    }
   ],
   "source": [
    "from utils import load_pkl\n",
    "\n",
    "path = '/Users/mhendriksen/Desktop/repositories/datasets/CUB_200_2011/best_model_CUB_30E_128BS_image_to_text.pkl'\n",
    "\n",
    "df = load_pkl(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i2t_query_ids</th>\n",
       "      <th>i2t_recalls_at_1</th>\n",
       "      <th>i2t_recalls_at_5</th>\n",
       "      <th>i2t_recalls_at_10</th>\n",
       "      <th>i2t_recalls_at_50</th>\n",
       "      <th>i2t_recalls_at_100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2938.000000</td>\n",
       "      <td>2938.000000</td>\n",
       "      <td>2938.000000</td>\n",
       "      <td>2938.000000</td>\n",
       "      <td>2938.000000</td>\n",
       "      <td>2938.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>58509.341389</td>\n",
       "      <td>0.001021</td>\n",
       "      <td>0.003063</td>\n",
       "      <td>0.006127</td>\n",
       "      <td>0.024506</td>\n",
       "      <td>0.052076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>33579.145458</td>\n",
       "      <td>0.031944</td>\n",
       "      <td>0.055272</td>\n",
       "      <td>0.078046</td>\n",
       "      <td>0.154642</td>\n",
       "      <td>0.222219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>29308.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>58722.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>87608.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>116554.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       i2t_query_ids  i2t_recalls_at_1  i2t_recalls_at_5  i2t_recalls_at_10  \\\n",
       "count    2938.000000       2938.000000       2938.000000        2938.000000   \n",
       "mean    58509.341389          0.001021          0.003063           0.006127   \n",
       "std     33579.145458          0.031944          0.055272           0.078046   \n",
       "min         0.000000          0.000000          0.000000           0.000000   \n",
       "25%     29308.500000          0.000000          0.000000           0.000000   \n",
       "50%     58722.000000          0.000000          0.000000           0.000000   \n",
       "75%     87608.250000          0.000000          0.000000           0.000000   \n",
       "max    116554.000000          1.000000          1.000000           1.000000   \n",
       "\n",
       "       i2t_recalls_at_50  i2t_recalls_at_100  \n",
       "count        2938.000000         2938.000000  \n",
       "mean            0.024506            0.052076  \n",
       "std             0.154642            0.222219  \n",
       "min             0.000000            0.000000  \n",
       "25%             0.000000            0.000000  \n",
       "50%             0.000000            0.000000  \n",
       "75%             0.000000            0.000000  \n",
       "max             1.000000            1.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Watch out, len(predictions) < k: 3 < 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def recall_at_k(target, predictions, k=5, total_in_collection=1):\n",
    "    if len(predictions) < k:\n",
    "        print(f'Watch out, len(predictions) < k: {len(predictions)} < {k}')\n",
    "    recall_at_k = predictions[:k].count(target) / total_in_collection\n",
    "    return 100 * recall_at_k\n",
    "\n",
    "\n",
    "t = 5\n",
    "preds = [1,3,5]\n",
    "\n",
    "recall_at_k(t, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eval_status</th>\n",
       "      <th>image_caption</th>\n",
       "      <th>image_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>Two young guys with shaggy hair look at their ...</td>\n",
       "      <td>1000092795.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>train</td>\n",
       "      <td>Several men in hard hats are operating a giant...</td>\n",
       "      <td>10002456.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>train</td>\n",
       "      <td>A child in a pink dress is climbing up a set o...</td>\n",
       "      <td>1000268201.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>train</td>\n",
       "      <td>Someone in a blue shirt and hat is standing on...</td>\n",
       "      <td>1000344755.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>train</td>\n",
       "      <td>Two men, one in a gray shirt, one in a black s...</td>\n",
       "      <td>1000366164.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155045</th>\n",
       "      <td>train</td>\n",
       "      <td>Woman writing on a pad in room with gold, deco...</td>\n",
       "      <td>997338199.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155050</th>\n",
       "      <td>train</td>\n",
       "      <td>A person in a red shirt climbing up a rock fac...</td>\n",
       "      <td>997722733.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155055</th>\n",
       "      <td>train</td>\n",
       "      <td>Two male construction workers are working on a...</td>\n",
       "      <td>997876722.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155060</th>\n",
       "      <td>train</td>\n",
       "      <td>An older busker in glasses plays an Eastern st...</td>\n",
       "      <td>99804383.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155065</th>\n",
       "      <td>train</td>\n",
       "      <td>A man in shorts and a Hawaiian shirt leans ove...</td>\n",
       "      <td>998845445.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31014 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       eval_status                                      image_caption  \\\n",
       "0            train  Two young guys with shaggy hair look at their ...   \n",
       "5            train  Several men in hard hats are operating a giant...   \n",
       "10           train  A child in a pink dress is climbing up a set o...   \n",
       "15           train  Someone in a blue shirt and hat is standing on...   \n",
       "20           train  Two men, one in a gray shirt, one in a black s...   \n",
       "...            ...                                                ...   \n",
       "155045       train  Woman writing on a pad in room with gold, deco...   \n",
       "155050       train  A person in a red shirt climbing up a rock fac...   \n",
       "155055       train  Two male construction workers are working on a...   \n",
       "155060       train  An older busker in glasses plays an Eastern st...   \n",
       "155065       train  A man in shorts and a Hawaiian shirt leans ove...   \n",
       "\n",
       "            image_name  \n",
       "0       1000092795.jpg  \n",
       "5         10002456.jpg  \n",
       "10      1000268201.jpg  \n",
       "15      1000344755.jpg  \n",
       "20      1000366164.jpg  \n",
       "...                ...  \n",
       "155045   997338199.jpg  \n",
       "155050   997722733.jpg  \n",
       "155055   997876722.jpg  \n",
       "155060    99804383.jpg  \n",
       "155065   998845445.jpg  \n",
       "\n",
       "[31014 rows x 3 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('/Users/mhendriksen/Desktop/repositories/datasets/flickr30k_images/dataset_flickr30k.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/mhendriksen/Desktop/repositories/datasets/fashion-200k/fashion200k.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201838"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.index.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172049"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.index.unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################################################\n",
    "# # attempt to reproduce clip loss as in the paper\n",
    "# ################################################\n",
    "# temperature = 1\n",
    "# t = temperature\n",
    "# n = batch_size = 3\n",
    "\n",
    "# # extract feature representations of each modality\n",
    "# I_f = torch.rand(n, 768)\n",
    "# T_f = torch.rand(n, 768)\n",
    "\n",
    "\n",
    "# # joint multimodal embedding\n",
    "# I_e = l2_normalize(projection.forward(I_f))\n",
    "# T_e = l2_normalize(projection.forward(T_f))\n",
    "\n",
    "\n",
    "# # scaled pairwise cosine similarities [n, n]\n",
    "# I_e = I_e.detach().numpy()\n",
    "# T_e = T_e.detach().numpy()\n",
    "# logits = np.dot(I_e, T_e.T) * np.exp(t)\n",
    "\n",
    "\n",
    "# # import numpy as np\n",
    "# # # symmetric loss functio\n",
    "# # labels = np.arange(n)\n",
    "# # loss_image = cross_entropy(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': 'CUB_200', 'dataset_root': '/Users/mhendriksen/Desktop/repositories/datasets/CUB_200_2011/', 'csv_file': 'cub_1_cap_per_img.csv', 'image_folder': 'CUB_200_2011/images', 'columns_dtypes': {'image_file': 'str', 'class': 'str', 'eval_status': 'str', 'caption': 'str'}, 'content_type': {'image': 'image_file', 'text': 'caption'}, 'clip_version': 'clip-ViT-L-14', 'dict_file': 'test_dict.pkl'}\n",
      "{'DATASET': {'CUB': {'text_dict_path': '/Users/mhendriksen/Desktop/repositories/datasets/CUB_200_2011/text_dict.pkl', 'image_dict_path': '/Users/mhendriksen/Desktop/repositories/datasets/CUB_200_2011/image_dict.pkl', 'df_train_path': '../data/CUB/df_train.csv', 'df_test_path': '../data/CUB/df_test.csv'}}, 'clip_version': 'clip-ViT-L-14', 'deliverables_folder': '../deliverables', 'batch_size': 128, 'epochs': 30, 'device': 'to be defined in the script', 'head_lr': 0.001, 'weight_decay': 0.001, 'patience': 1, 'factor': 0.8, 'text_input_dim': 768, 'text_hidden_dim': 2048, 'image_input_dim': 768, 'image_hidden_dim': 2048, 'projection_output_dim': 2048, 'dropout': 0.1, 'temperature': 1}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import torch\n",
    "\n",
    "############################\n",
    "# load config files\n",
    "############################\n",
    "train_config_file = '/Users/mhendriksen/Desktop/repositories/CLIP/conf/train_conf.yaml'\n",
    "dataset_config_file = '/Users/mhendriksen/Desktop/repositories/CLIP/conf/local_cub_conf.yaml'\n",
    "\n",
    "with open(dataset_config_file) as file:\n",
    "    config_ds = yaml.safe_load(file)\n",
    "print(config_ds)\n",
    "\n",
    "with open(train_config_file) as file:\n",
    "    config_train = yaml.safe_load(file)\n",
    "print(config_train)\n",
    "\n",
    "config_train[\"device\"] = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'CUB'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-04406476f344>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconfig_ds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CUB'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'CUB'"
     ]
    }
   ],
   "source": [
    "config_ds['CUB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "\n",
    "# # read df\n",
    "# df = pd.read_csv(\n",
    "#     filepath_or_buffer=os.path.join(config_ds['dataset_root'], config_ds['csv_file']),\n",
    "#     dtype=config_ds['columns_dtypes'],\n",
    "#     index_col=0\n",
    "\n",
    "#     )\n",
    "# df.head()\n",
    "\n",
    "# from utils import make_train_test_dev_dfs\n",
    "# # build train and valid loaders\n",
    "# df_train, df_test, df_dev = make_train_test_dev_dfs(df)\n",
    "# TODO: save the dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of released CLIP models as of 18 Aug 2022:\n",
    "\n",
    "| Model         | Top 1 Performance |\n",
    "| :------------ | :---------------- |\n",
    "| clip-ViT-B-32 | 63.3              |\n",
    "| clip-ViT-B-16 | 68.1              |\n",
    "| clip-ViT-L-14 | 75.4              |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "clip_model = SentenceTransformer('clip-ViT-L-14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from  /Users/mhendriksen/Desktop/repositories/datasets/CUB_200_2011/image_dict.pkl\n",
      "Loaded data from  /Users/mhendriksen/Desktop/repositories/datasets/CUB_200_2011/text_dict.pkl\n"
     ]
    }
   ],
   "source": [
    "text_dict_path = '/Users/mhendriksen/Desktop/repositories/datasets/CUB_200_2011/text_dict.pkl'\n",
    "image_dict_path = '/Users/mhendriksen/Desktop/repositories/datasets/CUB_200_2011/image_dict.pkl'\n",
    "\n",
    "from utils import load_pkl\n",
    "\n",
    "image_dict = load_pkl(image_dict_path)\n",
    "text_dict = load_pkl(text_dict_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv('df_train.csv', index_col=0)\n",
    "df_test = pd.read_csv('df_test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import ImageCaptionDataset, build_loader\n",
    "\n",
    "train_loader = build_loader(\n",
    "    dataset=ImageCaptionDataset,\n",
    "    config=config_train,\n",
    "    dataf=df_train,\n",
    "    text_dict=text_dict,\n",
    "    image_dict=image_dict,\n",
    "    mode='train'\n",
    "    )\n",
    "valid_loader = build_loader(\n",
    "    dataset=ImageCaptionDataset,\n",
    "    config=config_train,\n",
    "    dataf=df_test,\n",
    "    text_dict=text_dict,\n",
    "    image_dict=image_dict,\n",
    "    mode='test'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import CLIPModel\n",
    "import itertools\n",
    "# from util import cross_entropy\n",
    "\n",
    "model = CLIPModel(config=config_train).to(config_train[\"device\"])\n",
    "\n",
    "# get optimizer\n",
    "params = [\n",
    "    {\n",
    "        'params': itertools.chain(\n",
    "            model.image_projection_head.parameters(),\n",
    "            model.text_projection_head.parameters()\n",
    "        ),\n",
    "        'lr': config_train['head_lr'],\n",
    "        'weight_decay': config_train['weight_decay']\n",
    "    }\n",
    "]\n",
    "optimizer = torch.optim.AdamW(params)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode=\"min\",\n",
    "    patience=config_train[\"patience\"],\n",
    "    factor=config_train[\"factor\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lr_scheduler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-6620e37fd76e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'lr_scheduler' is not defined"
     ]
    }
   ],
   "source": [
    "lr_scheduler.step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:28<00:00,  6.60it/s, lr=0.001, train_loss=3.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3076368127618587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:02<00:00, 43.60it/s, valid_loss=3.45]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Best Model! best loss - 3.4483796575156247\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:28<00:00,  6.58it/s, lr=0.0008, train_loss=3.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2985007851212114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:02<00:00, 43.21it/s, valid_loss=3.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Best Model! best loss - 3.4434459783783815\n"
     ]
    }
   ],
   "source": [
    "from train_valid import train_epoch, valid_epoch\n",
    "\n",
    "step = \"epoch\"\n",
    "train_loss_values = []\n",
    "valid_loss_values = []\n",
    "best_loss = float('inf')\n",
    "for epoch in range(config_train[\"epochs\"]):\n",
    "    running_loss = 0.0\n",
    "    print(f\"Epoch: {epoch + 1}\")\n",
    "    model.train()\n",
    "    train_loss = train_epoch(config_train, model, train_loader, optimizer, lr_scheduler, step)\n",
    "    # print(train_loss.avg)\n",
    "    train_loss_values.append(train_loss.avg)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = valid_epoch(config_train, model, valid_loader)\n",
    "        valid_loss_values.append(valid_loss.avg)\n",
    "\n",
    "    if valid_loss.avg < best_loss:\n",
    "        best_loss = valid_loss.avg\n",
    "\n",
    "        # dt_string = get_dt_string()\n",
    "        torch.save(model.state_dict(), \"test_model.pt\")\n",
    "        print(f\"Saved Best Model! best loss - {best_loss}\")\n",
    "        # torch.save(valid_loader, f\"{CFG.deliverables_path}/{dt_string}_valid_loader.pth\")\n",
    "        # print('Saved the dataloader!')\n",
    "\n",
    "    lr_scheduler.step(valid_loss.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation pipeline\n",
    "from sentence_transformers import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from  /Users/mhendriksen/Desktop/repositories/datasets/CUB_200_2011/image_dict.pkl\n",
      "Loaded data from  /Users/mhendriksen/Desktop/repositories/datasets/CUB_200_2011/text_dict.pkl\n",
      "{'CUB': {'dataset_root': '/Users/mhendriksen/Desktop/repositories/datasets/CUB_200_2011/', 'csv_file': 'cub_1_cap_per_img.csv', 'image_folder': 'CUB_200_2011/images', 'columns_dtypes': {'image_file': 'str', 'class': 'str', 'eval_status': 'str', 'caption': 'str'}, 'content_type': {'image': 'image_file', 'text': 'caption'}, 'clip_version': 'clip-ViT-L-14', 'dict_file': 'test_dict.pkl'}}\n"
     ]
    }
   ],
   "source": [
    "import pickle, yaml\n",
    "import torch\n",
    "\n",
    "def load_pkl(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        print('Loaded data from ', path)\n",
    "    return data\n",
    "\n",
    "text_dict_path = '/Users/mhendriksen/Desktop/repositories/datasets/CUB_200_2011/text_dict.pkl'\n",
    "image_dict_path = '/Users/mhendriksen/Desktop/repositories/datasets/CUB_200_2011/image_dict.pkl'\n",
    "\n",
    "image_dict = load_pkl(image_dict_path)\n",
    "text_dict = load_pkl(text_dict_path)\n",
    "\n",
    "dataset_config_file = '/Users/mhendriksen/Desktop/repositories/CLIP/conf/local_cub_conf.yaml'\n",
    "with open(dataset_config_file) as file:\n",
    "    config_ds = yaml.safe_load(file)\n",
    "print(config_ds)\n",
    "\n",
    "eval_config_file = '../conf/train_conf.yaml'\n",
    "with open(eval_config_file) as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "config[\"device\"] = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = config['DATASET']['CUB']['model_path']\n",
    "\n",
    "os.path.exists(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/mhendriksen/Desktop/repositories/datasets/CUB_200_2011/best_model_CUB_30E_128BS.pt'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained model...\n"
     ]
    }
   ],
   "source": [
    "from model import CLIPModel\n",
    "\n",
    "\n",
    "print('Loading pre-trained model...')\n",
    "model_path = '/Users/mhendriksen/Desktop/repositories/datasets/CUB_200_2011/best_model_CUB_30E_128BS.pt'\n",
    "\n",
    "def get_model(path, config):\n",
    "    model = CLIPModel(config)\n",
    "    model.load_state_dict(torch.load(path, map_location=config[\"device\"]))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "model = get_model(path=model_path, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load dev df...\n",
      "Get ids, images and captions from dev df... \n"
     ]
    }
   ],
   "source": [
    "# get queries\n",
    "# get image\n",
    "\n",
    "dataset = 'CUB'\n",
    "\n",
    "# load dev df\n",
    "\n",
    "print('Load dev df...')\n",
    "df_dev_path = '/Users/mhendriksen/Desktop/repositories/CLIP/data/CUB/df_dev.csv'\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_dev = pd.read_csv(df_dev_path, index_col=0)\n",
    "\n",
    "print('Get ids, images and captions from dev df... ')\n",
    "ids = df_dev.index.tolist()\n",
    "images = df_dev[config_ds[dataset]['content_type']['image']].tolist()\n",
    "captions = df_dev[config_ds[dataset]['content_type']['text']].tolist()\n",
    "\n",
    "assert len(images) == len(captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text to image retrieval results...\n",
      "Build a list of textual query embeddings and an images tensor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2938/2938 [00:05<00:00, 574.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got a list of 2938 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2938/2938 [00:04<00:00, 612.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final tensor shape: torch.Size([2938, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from evaluation_utils import dict_to_tensor, dict_to_list, get_top_k_indices\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "print('Text to image retrieval results...')\n",
    "print('Build a list of textual query embeddings and an images tensor...')\n",
    "text_query_embs = dict_to_list(\n",
    "    indices=ids,\n",
    "    dict=text_dict,\n",
    "    trained_model=model,\n",
    "    content_type='text'\n",
    ")\n",
    "images_tensor = dict_to_tensor(\n",
    "    indices=ids,\n",
    "    dict=image_dict,\n",
    "    trained_model=model,\n",
    "    content_type='image',\n",
    "    dim=0,\n",
    "    clip_version='clip-ViT-L-14'\n",
    ")\n",
    "assert len(text_query_embs) == len(images_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'best_model_CUB_30E_128BS'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.basename(os.path.splitext(model_path)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2938, 2048])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.squeeze(images_tensor, dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768])\n",
      "torch.Size([1, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 2048])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluation_utils import numpy_to_tensor\n",
    "\n",
    "\n",
    "t = torch.from_numpy(image_dict['data'][0]['clip-ViT-L-14'])\n",
    "print(t.shape)\n",
    "t = torch.unsqueeze(t,0)\n",
    "print(t.shape)\n",
    "t = model.image_projection_head(t)\n",
    "\n",
    "l = [t, t]\n",
    "\n",
    "torch.stack(l, dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 768])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluation_utils import numpy_to_tensor\n",
    "\n",
    "\n",
    "test = torch.from_numpy(image_dict['data'][0]['clip-ViT-L-14'])\n",
    "\n",
    "#test = torch.unsqueeze(test, 0)\n",
    "\n",
    "tmp = [test, test]\n",
    "\n",
    "numpy_to_tensor(tmp, dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For each text query, retrieve top k=1000 results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2938it [00:36, 79.75it/s]\n"
     ]
    }
   ],
   "source": [
    "from evaluation_utils import get_top_k_indices, recall_at_k, get_top_k_matches_for_queries\n",
    "\n",
    "k=1000\n",
    "print(f'For each text query, retrieve top k={k} results...')\n",
    "text_to_image_results = get_top_k_matches_for_queries(\n",
    "    indices=ids,\n",
    "    queries_embs=text_query_embs,\n",
    "    target_tensor=images_tensor,\n",
    "    k=k,\n",
    "    normalized=True\n",
    ")\n",
    "assert len(text_to_image_results) == len(ids) == len(images_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating recall@k for text-to-image results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2938it [00:00, 131507.02it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t2i_query_ids</th>\n",
       "      <th>t2i_recalls_at_1</th>\n",
       "      <th>t2i_recalls_at_5</th>\n",
       "      <th>t2i_recalls_at_10</th>\n",
       "      <th>t2i_recalls_at_50</th>\n",
       "      <th>t2i_recalls_at_100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2938.000000</td>\n",
       "      <td>2938.000000</td>\n",
       "      <td>2938.000000</td>\n",
       "      <td>2938.000000</td>\n",
       "      <td>2938.000000</td>\n",
       "      <td>2938.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>58509.341389</td>\n",
       "      <td>0.000681</td>\n",
       "      <td>0.003063</td>\n",
       "      <td>0.005786</td>\n",
       "      <td>0.027910</td>\n",
       "      <td>0.061607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>33579.145458</td>\n",
       "      <td>0.026086</td>\n",
       "      <td>0.055272</td>\n",
       "      <td>0.075860</td>\n",
       "      <td>0.164743</td>\n",
       "      <td>0.240480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>29308.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>58722.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>87608.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>116554.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       t2i_query_ids  t2i_recalls_at_1  t2i_recalls_at_5  t2i_recalls_at_10  \\\n",
       "count    2938.000000       2938.000000       2938.000000        2938.000000   \n",
       "mean    58509.341389          0.000681          0.003063           0.005786   \n",
       "std     33579.145458          0.026086          0.055272           0.075860   \n",
       "min         0.000000          0.000000          0.000000           0.000000   \n",
       "25%     29308.500000          0.000000          0.000000           0.000000   \n",
       "50%     58722.000000          0.000000          0.000000           0.000000   \n",
       "75%     87608.250000          0.000000          0.000000           0.000000   \n",
       "max    116554.000000          1.000000          1.000000           1.000000   \n",
       "\n",
       "       t2i_recalls_at_50  t2i_recalls_at_100  \n",
       "count        2938.000000         2938.000000  \n",
       "mean            0.027910            0.061607  \n",
       "std             0.164743            0.240480  \n",
       "min             0.000000            0.000000  \n",
       "25%             0.000000            0.000000  \n",
       "50%             0.000000            0.000000  \n",
       "75%             0.000000            0.000000  \n",
       "max             1.000000            1.000000  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluation_utils import get_mrr, recall_at_k, get_recalls, iterate_over_results_to_compute_recalls\n",
    "\n",
    "print('Calculating recall@k for text-to-image results...')\n",
    "t2i_query_ids, t2i_top_k_results, t2i_recalls_at_1, t2i_recalls_at_5, t2i_recalls_at_10, t2i_recalls_at_50, t2i_recalls_at_100 = iterate_over_results_to_compute_recalls(\n",
    "    ids=ids,\n",
    "    results=text_to_image_results,\n",
    "    k_list=[1, 5, 10, 50, 100]\n",
    ")\n",
    "\n",
    "t2i_results = pd.DataFrame(\n",
    "    data={\n",
    "       't2i_query_ids': t2i_query_ids,\n",
    "        # text to image\n",
    "        't2i_top_k_results': t2i_top_k_results,\n",
    "        't2i_recalls_at_1': t2i_recalls_at_1,\n",
    "        't2i_recalls_at_5': t2i_recalls_at_5,\n",
    "        't2i_recalls_at_10': t2i_recalls_at_10,\n",
    "        't2i_recalls_at_50': t2i_recalls_at_50,\n",
    "        't2i_recalls_at_100': t2i_recalls_at_100,\n",
    "    }\n",
    ")\n",
    "\n",
    "t2i_results.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image to text retrieval results...\n",
      "Build a list of visual query embeddings and a captions tensor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2938/2938 [00:00<00:00, 300323.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got a list of 2938 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2938/2938 [00:00<00:00, 316384.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final tensor shape: torch.Size([2938, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('Image to text retrieval...')\n",
    "print('Build a list of visual query embeddings and a captions tensor...')\n",
    "image_query_embs = dict_to_list(\n",
    "    indices=ids,\n",
    "    dict=image_dict\n",
    ")\n",
    "text_tensor = dict_to_tensor(\n",
    "    indices=ids,\n",
    "    dict=text_dict,\n",
    "    clip_version='clip-ViT-L-14'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For each image query, retrieve top k=1000 captions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2938it [00:20, 140.38it/s]\n"
     ]
    }
   ],
   "source": [
    "from evaluation_utils import get_top_k_indices, recall_at_k, get_top_k_matches_for_queries\n",
    "\n",
    "k=1000\n",
    "print(f'For each image query, retrieve top k={k} captions...')\n",
    "image_to_text_results = get_top_k_matches_for_queries(\n",
    "    indices=ids,\n",
    "    queries_embs=image_query_embs,\n",
    "    target_tensor=text_tensor,\n",
    "    k=k,\n",
    "    normalized=True\n",
    ")\n",
    "assert len(image_to_text_results) == len(ids) == len(text_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating recall@k for image-to-text results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2938it [00:00, 109332.49it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i2t_query_ids</th>\n",
       "      <th>i2t_recalls_at_1</th>\n",
       "      <th>i2t_recalls_at_5</th>\n",
       "      <th>i2t_recalls_at_10</th>\n",
       "      <th>i2t_recalls_at_50</th>\n",
       "      <th>i2t_recalls_at_100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2938.000000</td>\n",
       "      <td>2938.000000</td>\n",
       "      <td>2938.000000</td>\n",
       "      <td>2938.000000</td>\n",
       "      <td>2938.000000</td>\n",
       "      <td>2938.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>58509.341389</td>\n",
       "      <td>0.012253</td>\n",
       "      <td>0.034037</td>\n",
       "      <td>0.055140</td>\n",
       "      <td>0.165759</td>\n",
       "      <td>0.256978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>33579.145458</td>\n",
       "      <td>0.110033</td>\n",
       "      <td>0.181354</td>\n",
       "      <td>0.228291</td>\n",
       "      <td>0.371927</td>\n",
       "      <td>0.437041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>29308.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>58722.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>87608.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>116554.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       i2t_query_ids  i2t_recalls_at_1  i2t_recalls_at_5  i2t_recalls_at_10  \\\n",
       "count    2938.000000       2938.000000       2938.000000        2938.000000   \n",
       "mean    58509.341389          0.012253          0.034037           0.055140   \n",
       "std     33579.145458          0.110033          0.181354           0.228291   \n",
       "min         0.000000          0.000000          0.000000           0.000000   \n",
       "25%     29308.500000          0.000000          0.000000           0.000000   \n",
       "50%     58722.000000          0.000000          0.000000           0.000000   \n",
       "75%     87608.250000          0.000000          0.000000           0.000000   \n",
       "max    116554.000000          1.000000          1.000000           1.000000   \n",
       "\n",
       "       i2t_recalls_at_50  i2t_recalls_at_100  \n",
       "count        2938.000000         2938.000000  \n",
       "mean            0.165759            0.256978  \n",
       "std             0.371927            0.437041  \n",
       "min             0.000000            0.000000  \n",
       "25%             0.000000            0.000000  \n",
       "50%             0.000000            0.000000  \n",
       "75%             0.000000            1.000000  \n",
       "max             1.000000            1.000000  "
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluation_utils import get_mrr, recall_at_k, get_recalls, iterate_over_results_to_compute_recalls\n",
    "\n",
    "print('Calculating recall@k for image-to-text results...')\n",
    "i2t_query_ids, i2t_top_k_results, i2t_recalls_at_1, i2t_recalls_at_5, i2t_recalls_at_10, i2t_recalls_at_50, i2t_recalls_at_100 = iterate_over_results_to_compute_recalls(\n",
    "    ids=ids,\n",
    "    results=image_to_text_results,\n",
    "    k_list=[1, 5, 10, 50, 100]\n",
    ")\n",
    "\n",
    "i2t_results = pd.DataFrame(\n",
    "    data={\n",
    "       'i2t_query_ids': i2t_query_ids,\n",
    "        # text to image\n",
    "        'i2t_top_k_results': i2t_top_k_results,\n",
    "        'i2t_recalls_at_1': i2t_recalls_at_1,\n",
    "        'i2t_recalls_at_5': i2t_recalls_at_5,\n",
    "        'i2t_recalls_at_10': i2t_recalls_at_10,\n",
    "        'i2t_recalls_at_50': i2t_recalls_at_50,\n",
    "        'i2t_recalls_at_100': i2t_recalls_at_100,\n",
    "    }\n",
    ")\n",
    "\n",
    "i2t_results.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91a6f007d1f64537ae6d0311688c9d13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from evaluation_utils import recall_at_k, get_mrr\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# iterate over the retrieved results,\n",
    "# compute mrr, recall@k where k = {1, 5, 10, 50, 100}\n",
    "k = 100\n",
    "for query_id, result in tqdm(zip(query_ids, retrieved_results)):\n",
    "\n",
    "    recall_at_100 = recall_at_k(\n",
    "        target=query_id,\n",
    "        predictions=result,\n",
    "        k=k,\n",
    "        total_in_collection=1\n",
    "    )\n",
    "    mrr = get_mrr(\n",
    "        target=query_id,\n",
    "        predictions=result\n",
    "    )\n",
    "    if recall_at_100 > 0.0:\n",
    "        print(recall_at_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class RecallMeter:\n",
    "    def __init__(self, target, predictions, total_in_collection=1, name='T2i| i2t recall'):\n",
    "        self.target = target\n",
    "        self.predictions = predictions\n",
    "        self.total_in_collection = total_in_collection\n",
    "        self.name = name\n",
    "\n",
    "    def recall_at_k(self, k=5):\n",
    "        self.predictions[:k].count(self.target) / self.total_in_collection\n",
    "\n",
    "    def __repr__(self):\n",
    "        text = f\"{self.name}: {self.avg:.4f}\"\n",
    "        return text\n",
    "\n",
    "\n",
    "t2i_recall = RecallMeter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[53619, 53699, 2160, 1866, 10006]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F\n",
    "\n",
    "normalized = True\n",
    "k=5\n",
    "\n",
    "\n",
    "image_index = numpy_to_tensor(image_index)\n",
    "matches, values = get_top_k_indices(\n",
    "    query_emb=query_emb,\n",
    "    index=ids,\n",
    "    target_embeddings=image_index,\n",
    "    k=k,\n",
    "    normalized=True\n",
    "    )\n",
    "\n",
    "predictions = df_dev.loc[matches]['image_file'].tolist()\n",
    "\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id in matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53619    this bird is mostly grey with a black bill, bl...\n",
       "53699    this bird has a white crown, a long black bill...\n",
       "2160     all black bird with black bill and white tippe...\n",
       "1866     the bird has a very large beak almost the size...\n",
       "10006    this bird has a small beak and head compared t...\n",
       "Name: caption, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev.loc[matches]['caption']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a large gray bird with a long wingspan and a long black beak.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fcd56de5e8ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# make an images index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclip_version\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'image_dict' is not defined"
     ]
    }
   ],
   "source": [
    "# text2image retrieval\n",
    "# make an images index\n",
    "d = []\n",
    "a = image_dict['data'][0][clip_version]\n",
    "d.append(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('multimod')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7cf8fa6a4d2f1a7413d859f5f981d96e21ad415222169f422bb85d24d587037a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
