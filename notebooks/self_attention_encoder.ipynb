{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append('/Users/mhendriksen/Desktop/repositories/CLIP/src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from  /Users/mhendriksen/Desktop/repositories/datasets/CUB_200_2011/image_dict.pkl\n",
      "Loaded data from  /Users/mhendriksen/Desktop/repositories/datasets/CUB_200_2011/text_dict.pkl\n"
     ]
    }
   ],
   "source": [
    "text_dict_path = '/Users/mhendriksen/Desktop/repositories/datasets/CUB_200_2011/text_dict.pkl'\n",
    "image_dict_path = '/Users/mhendriksen/Desktop/repositories/datasets/CUB_200_2011/image_dict.pkl'\n",
    "\n",
    "from utils import load_pkl\n",
    "\n",
    "image_dict = load_pkl(image_dict_path)\n",
    "text_dict = load_pkl(text_dict_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CUB': {'dataset_root': '/Users/mhendriksen/Desktop/repositories/datasets/CUB_200_2011/', 'csv_file': 'cub_1_cap_per_img.csv', 'image_folder': 'CUB_200_2011/images', 'columns_dtypes': {'image_file': 'str', 'class': 'str', 'eval_status': 'str', 'caption': 'str'}, 'content_type': {'image': 'image_file', 'text': 'caption'}, 'clip_version': 'clip-ViT-L-14', 'dict_file': 'test_dict.pkl'}}\n",
      "{'DATASET': {'CUB': {'root': '/ivi/ilps/personal/mbiriuk/CUB_200_2011/', 'text_dict': 'text_dict.pkl', 'image_dict': 'image_dict.pkl', 'df_train': 'df_train.csv', 'df_test': 'df_test.csv', 'df_dev': 'df_dev.csv', 'model_path': '/Users/mhendriksen/Desktop/repositories/datasets/CUB_200_2011/best_model_CUB_30E_128BS.pt'}, 'ABO': {'root': '/ivi/ilps/personal/mbiriuk/abo', 'text_dict': 'text_dict.pkl', 'image_dict': 'image_dict.pkl', 'df_train': 'df_train.csv', 'df_test': 'df_test.csv'}, 'fashion200k': {'root': '/ivi/ilps/personal/mbiriuk/fashion200k', 'text_dict': 'text_dict.pkl', 'image_dict': 'image_dict.pkl', 'df_train': 'df_train.csv', 'df_test': 'df_test.csv'}, 'coco': {'root': '/ivi/ilps/personal/mbiriuk/ms-coco', 'text_dict': 'text_dict.pkl', 'image_dict': 'image_dict.pkl', 'df_train': 'df_train.csv', 'df_test': 'df_test.csv'}, 'f30k': {'root': '/ivi/ilps/personal/mbiriuk/flickr30k_images', 'text_dict': 'text_dict.pkl', 'image_dict': 'image_dict.pkl', 'df_train': 'df_train.csv', 'df_test': 'df_test.csv'}}, 'clip_version': 'clip-ViT-L-14', 'manual_seed': 0, 'deliverables_folder': 'deliverables', 'batch_size': 128, 'epochs': 30, 'device': 'to be defined in the script', 'num_workers': 4, 'head_lr': 0.001, 'weight_decay': 0.001, 'patience': 1, 'factor': 0.8, 'text_input_dim': 768, 'text_hidden_dim': 2048, 'image_input_dim': 768, 'image_hidden_dim': 2048, 'projection_output_dim': 2048, 'dropout': 0.1, 'temperature': 1}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import torch\n",
    "\n",
    "############################\n",
    "# load config files\n",
    "############################\n",
    "train_config_file = '/Users/mhendriksen/Desktop/repositories/CLIP/conf/train_conf.yaml'\n",
    "dataset_config_file = '/Users/mhendriksen/Desktop/repositories/CLIP/conf/local_cub_conf.yaml'\n",
    "\n",
    "with open(dataset_config_file) as file:\n",
    "    config_ds = yaml.safe_load(file)\n",
    "print(config_ds)\n",
    "\n",
    "with open(train_config_file) as file:\n",
    "    config_train = yaml.safe_load(file)\n",
    "print(config_train)\n",
    "\n",
    "config_train[\"device\"] = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_path = '/Users/mhendriksen/Desktop/repositories/CLIP/data/CUB/df_test.csv'\n",
    "df_train_path = '/Users/mhendriksen/Desktop/repositories/CLIP/data/CUB/df_test.csv'\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv(df_train_path, index_col=0)\n",
    "df_test = pd.read_csv(df_test_path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import ImageCaptionDataset, build_loader\n",
    "\n",
    "train_loader = build_loader(\n",
    "    dataset=ImageCaptionDataset,\n",
    "    config=config_train,\n",
    "    dataf=df_train,\n",
    "    text_dict=text_dict,\n",
    "    image_dict=image_dict,\n",
    "    mode='train'\n",
    "    )\n",
    "valid_loader = build_loader(\n",
    "    dataset=ImageCaptionDataset,\n",
    "    config=config_train,\n",
    "    dataf=df_test,\n",
    "    text_dict=text_dict,\n",
    "    image_dict=image_dict,\n",
    "    mode='test'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, texts = batch[0], batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-ff9fbad424e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mself_attn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultiheadAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mself_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/multimod/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/multimod/lib/python3.8/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask)\u001b[0m\n\u001b[1;32m    976\u001b[0m                 v_proj_weight=self.v_proj_weight)\n\u001b[1;32m    977\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m             return F.multi_head_attention_forward(\n\u001b[0m\u001b[1;32m    979\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_proj_bias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/multimod/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v)\u001b[0m\n\u001b[1;32m   4132\u001b[0m                 \u001b[0mq_proj_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mq_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_proj_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4133\u001b[0m                 v_proj_weight=v_proj_weight, static_k=static_k, static_v=static_v)\n\u001b[0;32m-> 4134\u001b[0;31m     \u001b[0mtgt_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4135\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0membed_dim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0membed_dim_to_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4136\u001b[0m     \u001b[0;31m# allow MHA to have different sizes for the feature dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "input_dim = 768\n",
    "num_heads = 8\n",
    "\n",
    "self_attn = nn.MultiheadAttention(embed_dim=input_dim, num_heads=num_heads)\n",
    "\n",
    "self_attn(images, images, images)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layer = nn.TransformerEncoderLayer(d_model=768, nhead=8)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "src = torch.rand(10, 5, 768)\n",
    "out = transformer_encoder(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2242, -1.0310, -0.2805,  ..., -0.8324, -1.2521,  0.1358],\n",
       "         [-0.7744, -1.7791,  0.4502,  ...,  0.9238, -1.2825,  0.6134],\n",
       "         [-0.6749, -0.2804,  0.1898,  ..., -0.3859, -0.5585,  0.9081],\n",
       "         ...,\n",
       "         [-0.3853, -1.8718,  0.6324,  ..., -0.2026, -0.3867,  0.3954],\n",
       "         [ 0.6329, -1.3164,  0.1597,  ...,  0.5353, -1.1203, -0.1306],\n",
       "         [-0.1416, -0.2096,  1.0230,  ..., -0.7071, -0.3981,  0.1989]],\n",
       "\n",
       "        [[ 0.1905, -1.4942, -0.1423,  ..., -0.3503, -0.5814, -0.0370],\n",
       "         [-0.2450, -1.6431,  0.1286,  ...,  0.2667, -1.4127,  0.7829],\n",
       "         [-0.1701, -0.1983,  0.1089,  ..., -0.4841, -0.3509,  0.8635],\n",
       "         ...,\n",
       "         [-0.9135, -1.2016, -0.1343,  ...,  0.5964, -0.2756,  1.9324],\n",
       "         [ 0.2297, -0.1577,  0.9443,  ...,  1.0557, -0.1343, -0.1161],\n",
       "         [-1.2275, -0.9398,  0.4853,  ...,  0.6244, -0.6457,  0.3253]],\n",
       "\n",
       "        [[ 0.1592, -1.6454,  0.6606,  ..., -0.3709, -0.4614, -0.3115],\n",
       "         [ 0.3075, -1.2871,  0.3138,  ..., -0.3506, -1.5763,  1.7260],\n",
       "         [ 0.1394, -1.1747, -0.0160,  ..., -0.7807, -0.3308,  0.8404],\n",
       "         ...,\n",
       "         [-0.2918, -1.2510,  0.0327,  ..., -0.1731, -0.3158,  0.5454],\n",
       "         [ 0.6934, -0.9621, -0.5577,  ...,  0.0343, -0.1406,  0.3297],\n",
       "         [-0.4293, -0.1823,  0.7368,  ...,  0.4797, -0.8314,  0.3559]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.0477, -1.2648,  0.1488,  ..., -0.0144, -0.6415,  0.9699],\n",
       "         [ 0.4828, -2.1083,  0.6399,  ...,  0.4007, -1.3883,  0.4147],\n",
       "         [-1.5595,  0.2978,  0.1279,  ..., -1.5004, -0.0253,  0.5152],\n",
       "         ...,\n",
       "         [-0.0221, -0.9136, -0.2072,  ...,  0.1232, -0.1777,  1.8513],\n",
       "         [ 0.4945, -0.4989, -0.4102,  ..., -0.6051, -1.4358,  0.7977],\n",
       "         [ 0.3329, -1.1470,  0.8292,  ..., -0.4406, -0.8145,  0.4023]],\n",
       "\n",
       "        [[ 0.1728, -0.7798,  0.5118,  ...,  0.1810, -0.7078,  0.0107],\n",
       "         [ 0.1646, -2.0547,  1.1609,  ...,  0.8236, -1.4794,  0.4600],\n",
       "         [ 0.3391, -1.4489, -0.2569,  ..., -0.7400, -1.2173,  0.8427],\n",
       "         ...,\n",
       "         [-0.5319, -0.6821,  1.0047,  ...,  0.0297, -1.4263,  0.1613],\n",
       "         [ 0.3344, -0.5760, -0.2037,  ..., -0.0827, -0.4948, -0.2254],\n",
       "         [-0.0234, -1.2743,  0.6503,  ..., -0.1464, -0.2582,  1.2683]],\n",
       "\n",
       "        [[ 1.0252, -1.5224,  0.4170,  ..., -0.8660,  0.2806,  0.5477],\n",
       "         [-0.2922, -2.1288,  1.0094,  ...,  0.7953, -1.1935,  0.9478],\n",
       "         [ 0.8687, -0.7889, -0.6264,  ..., -0.4139, -0.4636,  0.3423],\n",
       "         ...,\n",
       "         [-0.9705, -0.7102,  0.3183,  ...,  0.2643, -1.0690,  1.4436],\n",
       "         [ 0.4224, -0.7776,  0.7994,  ..., -0.2640, -0.4410,  0.4428],\n",
       "         [-0.3594, -0.7089,  0.1761,  ...,  0.1246, -0.6876,  0.8603]]],\n",
       "       grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, num_heads, dim_feedforward, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            input_dim - Dimensionality of the input\n",
    "            num_heads - Number of heads to use in the attention block\n",
    "            dim_feedforward - Dimensionality of the hidden layer in the MLP\n",
    "            dropout - Dropout probability to use in the dropout layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Attention layer\n",
    "        self.self_attn = nn.MultiheadAttention(input_dim, input_dim, num_heads)\n",
    "\n",
    "        # Two-layer MLP\n",
    "        self.linear_net = nn.Sequential(\n",
    "            nn.Linear(input_dim, dim_feedforward),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(dim_feedforward, input_dim)\n",
    "        )\n",
    "\n",
    "        # Layers to apply in between the main layers\n",
    "        self.norm1 = nn.LayerNorm(input_dim)\n",
    "        self.norm2 = nn.LayerNorm(input_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Attention part\n",
    "        attn_out = self.self_attn(x)\n",
    "        x = x + self.dropout(attn_out)\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # MLP part\n",
    "        linear_out = self.linear_net(x)\n",
    "        x = x + self.dropout(linear_out)\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 2 required positional arguments: 'key' and 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-43f3a4548dba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoderBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m768\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_feedforward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/multimod/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-b1be3808ba0d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# Attention part\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mattn_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/multimod/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 2 required positional arguments: 'key' and 'value'"
     ]
    }
   ],
   "source": [
    "block = EncoderBlock(input_dim=768, num_heads=8, dim_feedforward=2048)\n",
    "\n",
    "block(images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('multimod')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7cf8fa6a4d2f1a7413d859f5f981d96e21ad415222169f422bb85d24d587037a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
